{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "id": "Fg9cVqLhcv1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aw7Gnccccpp5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import wandb\n",
        "from wandb.keras import (\n",
        "   WandbMetricsLogger,\n",
        "   WandbModelCheckpoint,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run = wandb.init(project=\"Akhund and Human\")\n",
        "config = wandb.config\n",
        "wandb_callbacks = [\n",
        "   WandbMetricsLogger(log_freq=5),\n",
        "   WandbModelCheckpoint(\"models\"),\n",
        "]"
      ],
      "metadata": {
        "id": "tXgOQK1gc5HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/drive/MyDrive/Data/Akhund-and-Human\"\n",
        "width = height = 224\n",
        "\n",
        "idg = ImageDataGenerator(\n",
        "    rescale = 1./255,\n",
        "    horizontal_flip=True,\n",
        "    brightness_range=(0.8,1.2),\n",
        "    zoom_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    rotation_range=10,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "train_data = idg.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(width,height),\n",
        "    batch_size=32,\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_data = idg.flow_from_directory(\n",
        "    dataset_path,\n",
        "    target_size=(width,height),\n",
        "    batch_size=32,\n",
        "    subset='validation'\n",
        ")"
      ],
      "metadata": {
        "id": "7zQTEBYYdUmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "some_images=next(train_data)\n",
        "X=some_images[0]\n",
        "Y=some_images[1]\n",
        "\n",
        "plt.figure(figsize=(16,16))\n",
        "for i in range(32):\n",
        "  plt.subplot(4,8,i+1)\n",
        "  plt.imshow(X[i])\n"
      ],
      "metadata": {
        "id": "BMaZBzEPdne3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(width,height,3),\n",
        "    pooling='avg'\n",
        ")"
      ],
      "metadata": {
        "id": "-DIi4Jqsdwnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.summary()"
      ],
      "metadata": {
        "id": "xgaLru0md-Ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in base_model.layers[0:-4]:\n",
        "  layer.trainable = False"
      ],
      "metadata": {
        "id": "-KlQGJVUeBSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    base_model,\n",
        "    layers.Dropout(0.2),\n",
        "    layers.Dense(2,activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "kcQ86ZEdeEFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "    loss=tf.keras.losses.categorical_crossentropy,\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "metadata": {
        "id": "O15MAwLQeG-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "YwwiXOOWeVWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt_model = model.fit(train_data, validation_data=val_data,callbacks=wandb_callbacks, epochs=20)"
      ],
      "metadata": {
        "id": "g9RxQFfPebNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/drive/MyDrive/Data/Akhund_and_Human.h5\")\n"
      ],
      "metadata": {
        "id": "htsbnqGmhGT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images,labels = val_data[1]\n",
        "ypred_list = []\n",
        "for image in images:\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    ypred=model.predict(image)\n",
        "    ypred_list.append(np.argmax(ypred))\n",
        "ytrue_list = []\n",
        "for label in labels:\n",
        "    ytrue_list.append(np.argmax(label))"
      ],
      "metadata": {
        "id": "MiQO1Q-3hj3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "images = cv2.imread(\"/content/2.jpg\")\n",
        "images= cv2.cvtColor(images , cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(images)\n",
        "images = cv2.resize(images , (299,299))\n",
        "images = images / 255.0\n",
        "images = np.reshape(images, [1, 299, 299, 3])\n",
        "\n",
        "output = model.predict(images)\n",
        "predicted_class = np.argmax(output)\n",
        "list = list(train_data.class_indices)"
      ],
      "metadata": {
        "id": "xmaeObmjiBt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = cv2.imread(\"/content/about.png\")\n",
        "images= cv2.cvtColor(images , cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(images)\n",
        "images = cv2.resize(images , (299,299))\n",
        "images = images / 255.0\n",
        "images = np.reshape(images, [1, 299, 299, 3])\n",
        "\n",
        "output = model.predict(images)\n",
        "predicted_class = np.argmax(output)\n",
        "list = list(train_data.class_indices)\n",
        "print(\"label : \" , list[np.argmax(output)])"
      ],
      "metadata": {
        "id": "hA1_VDLDjVLH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}